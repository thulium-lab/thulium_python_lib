{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%pylab inline\n",
    "#from IPython.html.widgets import FloatProgress\n",
    "#from IPython.display import display\n",
    "from matplotlib.cbook import flatten\n",
    "from numpy import *\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#class for encoding numpy array to json\n",
    "class JsonCustomEncoder(json.JSONEncoder):\n",
    "    \"\"\"\n",
    "    class for encoding numpy array to json\n",
    "    \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (ndarray, number)):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (complex, complex)):\n",
    "            return [obj.real, obj.imag]\n",
    "        elif isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        elif isinstance(obj, bytes):  # pragma: py3\n",
    "            return obj.decode()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removed coeffitioent 2 in gaussian functions in exponents\n",
    "def gaussian(x,N,x0,sigma, background):\n",
    "    \"\"\"Returns value of a 1D-gaussian with the given parameters ,N,x0,sigma, background\"\"\"\n",
    "    #from numpy import sqrt,pi,exp\n",
    "    return N / (sigma * sqrt(pi)) * exp(-(x - x0)**2/(sigma**2)) + background\n",
    "\n",
    "def gaussian2D(N, x0, y0, sigma_x, sigma_y, background):\n",
    "    \"\"\"Returns a 2D-gaussian function with the given parameters x0, y0, sigma_x, sigma_y, background\"\"\"\n",
    "    #from numpy import pi,exp\n",
    "    sigma_x = float(sigma_x)\n",
    "    sigma_y = float(sigma_y)\n",
    "    return lambda x,y: N / (sigma_x * sigma_y  * pi) * exp(\n",
    "                        -(((x - x0) / sigma_x)**2 + ((y - y0) / sigma_y)**2)) + background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Image_Basics():\n",
    "    \"\"\"Basic image processing, with only center determination and background substructing. \n",
    "        Fits are available and called later.\n",
    "        isgood parameter is needed for indicating if smth wrong with the image during its processing \n",
    "        (i.e. if image is blank)\"\"\"\n",
    "    def __init__(self,image, image_url='not_defined'):\n",
    "        self.image_url = image_url\n",
    "        self.image = image\n",
    "        c_pos = self.image.argmax()\n",
    "        self.center_pos = array([c_pos//self.image.shape[1], c_pos%self.image.shape[1]])\n",
    "        self.image = self.bgd_substract()\n",
    "        self.total = sum(self.image)\n",
    "        self.isgood = True\n",
    "    def bgd_substract(self, slice_to_c = (-20,-1)):\n",
    "        \"\"\" Substracts background, which is calculated using vertical strip at right side\"\"\"\n",
    "        data_for_bgd_det = self.image[:,slice_to_c[0]:slice_to_c[1]]\n",
    "        av = sum(data_for_bgd_det,1)/data_for_bgd_det.shape[1]\n",
    "        return self.image - tile(av[:,newaxis],self.image.shape[1])\n",
    "    def fit_gaussian1D(self, axis, width=10):\n",
    "        \"\"\"Returns (height, x, width_x,  background)\n",
    "        the gaussian parameters of a 2D distribution found by a fit\"\"\"\n",
    "        from scipy.optimize import curve_fit\n",
    "        data = sum(self.image, axis)\n",
    "        popt, pcov = curve_fit(gaussian, range(len(data)), data, p0=(self.total, argmax(data), width, 0))\n",
    "        return popt\n",
    "    def fit_gaussian2D(self):\n",
    "        \"\"\"Returns (height, y, x, width_y, width_x, background)\n",
    "        the gaussian parameters of a 2D distribution found by a fit\"\"\"\n",
    "        from scipy import optimize\n",
    "        params = (self.total, self.fit1D_y[1], self.fit1D_x[1], self.fit1D_y[2], self.fit1D_x[2], 0)\n",
    "        errorfunction = lambda p: ravel(gaussian2D(*p)(*indices(self.image.shape)) -\n",
    "                                 self.image)\n",
    "        popt, success = optimize.leastsq(errorfunction, params)\n",
    "        return popt\n",
    "    # removed coeffitioent 2 in gaussian functions in exponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Load_Image():\n",
    "    \"\"\" Class for loading image. \n",
    "        Constructor:\n",
    "        __init__(self, dview = None, do_fit1D_x=True, do_fit1D_y=True, do_fit2D=False, do_filtering=False)\n",
    "        Parameters are clear, dview - view on remote engins (to run in parallel)\n",
    "        To change filter function from default (gaussian_filter), do instance.filter_functions = new_function, it should take\n",
    "        only 1 parameter\n",
    "        \"\"\"\n",
    "    def __init__(self, dview = None, do_fit1D_x=True, do_fit1D_y=True, do_fit2D=False, do_filtering=False):\n",
    "        from scipy.ndimage import gaussian_filter#, median_filter\n",
    "        self.do_fit1D_x = do_fit1D_x\n",
    "        self.do_fit1D_y = do_fit1D_y\n",
    "        self.do_fit2D = do_fit2D\n",
    "        self.do_filtering = do_filtering\n",
    "        self.filter_function = gaussian_filter\n",
    "        self.filter_param = 1 # for gaussian filtering\n",
    "        dview['loader'] = self # loader - name of instance to load images, send this class to subengins for parallel\n",
    "    def load_image(self,image_url):\n",
    "        \"\"\" Loads individual image and performs fits. If fit wasn't found, isgood flag is set to False\n",
    "            If do_filtering is True, filters image\n",
    "            Depending on flags, adds follow attribute:\n",
    "            fit1Dx = [total, x0, sigma_x, background]\n",
    "            fit1Dy = [total, y0, sigma_y, background]\n",
    "            fit2D  = [total, y0, x0, sigma_y, sigma_x, background]\n",
    "        \"\"\"\n",
    "        from matplotlib.pyplot import imread\n",
    "        from re import findall\n",
    "        data = Image_Basics(imread(image_url),image_url)\n",
    "        (data.folderN, data.shotN, data.shot_typeN) = map(float, findall(r\"[-+]?\\d*\\.\\d+|\\d+\", image_url)[-3:])\n",
    "        if self.do_filtering:\n",
    "            data.image = self.filter_function(data.image,self.filter_param)\n",
    "        try: \n",
    "            if self.do_fit1D_x:\n",
    "                data.fit1D_x = data.fit_gaussian1D(0)\n",
    "            if self.do_fit1D_y:\n",
    "                data.fit1D_y = data.fit_gaussian1D(1)\n",
    "            if self.do_fit2D:\n",
    "                data.fit2D = data.fit_gaussian2D()\n",
    "        except RuntimeError:\n",
    "            print(\"RuntimeError, couldn't find fit for image\", data.image_url)\n",
    "            data.isgood = False\n",
    "        return data\n",
    "    def __call__(self,directory,lview = None):\n",
    "        \"\"\" Construct list of image_urls and load them in parallel. Bad images are not filtered out here, only indication\n",
    "            by isgood flag\n",
    "        \"\"\"\n",
    "        import os, re\n",
    "        dirs = [os.path.join(directory,dr) for dr in os.listdir(directory) if re.match(r'[-+]?[0-9.]+ms',dr)]\n",
    "        files_to_load = [os.path.join(dr,fl) for dr in dirs for fl in os.listdir(dr) if re.match(r'.*_\\d+.png',fl)]\n",
    "        res = lview.map(lambda x: self.load_image(x), files_to_load)\n",
    "        res.wait_interactive()\n",
    "        #all_data = list(flatten(res.result))\n",
    "        #all_data = list(flatten(map(self.load_image, files_to_load)))\n",
    "        print(''.join([x['stdout'] for x in res.metadata]))\n",
    "        print('Total number of images: ', len(res.result))\n",
    "        return res.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rearrange_data(all_data,sift_by_isgood=True):\n",
    "    \"\"\" Rearranges data from flat list to dictionary simultaneously dropping bad images if flag sift_by_isgood is set\n",
    "    to True (it can be set to False when signaks are weak)\n",
    "    \"\"\"\n",
    "    dataD = dict()\n",
    "#     w = FloatProgress(min=0, max=len(all_data),value=0)\n",
    "#     w.description='Rearranging in progress...'\n",
    "#     display(w)\n",
    "    for elem in all_data:\n",
    "#         w.value +=1\n",
    "        if sift_by_isgood and (not elem.isgood):\n",
    "            print('Not good ',elem.image_url)\n",
    "            continue\n",
    "        if elem.folderN not in dataD:\n",
    "            dataD[elem.folderN] = dict()\n",
    "        d = dataD[elem.folderN]\n",
    "        if elem.shot_typeN not in d:\n",
    "            d[elem.shot_typeN] = []\n",
    "        d[elem.shot_typeN].append(elem)\n",
    "#     w.bar_style='success'\n",
    "#     w.description = 'Rearranging Done'\n",
    "    print('Rearranging to dictionary is complited')\n",
    "    return dataD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Avr_Image():\n",
    "    \"\"\" Class for average data, has all attributes as Load_Image\n",
    "        \"\"\"\n",
    "    def __init__(self, dview=None, do_fit1D_x=True, do_fit1D_y=True, do_fit2D=True, do_filtering=False, do_sifting=True):\n",
    "        from scipy.ndimage import gaussian_filter#, median_filter\n",
    "        self.do_fit1D_x = do_fit1D_x\n",
    "        self.do_fit1D_y = do_fit1D_y\n",
    "        self.do_fit2D = do_fit2D\n",
    "        self.do_sifting = do_sifting\n",
    "        self.do_filtering = do_filtering\n",
    "        self.filter_function = gaussian_filter\n",
    "        self.filter_param = 1 # for gaussian filtering\n",
    "        dview['averager']=self\n",
    "    def check_image(self,avr_image, image): \n",
    "        # check is based on comparison position of center of average image and individual image\n",
    "        conf_int = 0.1\n",
    "        x = image.isgood \\\n",
    "            and abs((avr_image.center_pos[0] - image.center_pos[0]) / avr_image.center_pos[0]) < conf_int \\\n",
    "            and abs((avr_image.center_pos[1] - image.center_pos[1]) / avr_image.center_pos[1]) < conf_int\n",
    "        return x\n",
    "    def avr_image(self,folderN, shot_typeN, image_list):\n",
    "        \"\"\" For average image there are (if) exist everaged data from each image and its standatd deviation:\n",
    "        data.total_mean, data.total_std, fit1D_x(y)_mean, fit1D_x(y)_std, data.fit2D_mean, data.fit2D_std\n",
    "        If any fit wasn't found avr_image is not added to the dictionary\"\"\"\n",
    "        data = Image_Basics(mean([d.image for d in image_list],0), \"folder=%f,shot_typeN=%i\"%(folderN,shot_typeN))\n",
    "        # sifting \n",
    "        if self.do_sifting:\n",
    "            # construct new image list with valid images\n",
    "            new_image_list = [image for image in image_list if self.check_image(data, image)]\n",
    "#             print(\"folder=%f,shot_typeN=%i\"%(folderN,shot_typeN), len(image_list),len(new_image_list))\n",
    "            if len(new_image_list) == 0:\n",
    "                print(\"All images are sifted in\" + data.image_url)\n",
    "                data.isgood = False\n",
    "                return []\n",
    "            else:\n",
    "                print(len(image_list) - len(new_image_list), 'images are sifted in', data.image_url)\n",
    "                image_list = new_image_list\n",
    "                # construct new average image from this new list\n",
    "                data = Image_Basics(mean([d.image for d in image_list],0), \"folder=%f,shot_typeN=%i\"%(folderN,shot_typeN))\n",
    "        try: \n",
    "            if self.do_fit1D_x:\n",
    "                data.fit1D_x = data.fit_gaussian1D(0)\n",
    "            if self.do_fit1D_y:\n",
    "                data.fit1D_y = data.fit_gaussian1D(1)\n",
    "            if self.do_fit2D:\n",
    "                data.fit2D = data.fit_gaussian2D()\n",
    "        except RuntimeError:\n",
    "            print(\"RuntimeError, couldn't find fit for image\", data.image_url)\n",
    "            data.isgood = False\n",
    "            return []\n",
    "        data.total_mean = mean([d.total for d in image_list],0)\n",
    "        data.total_std = std([d.total for d in image_list],0)\n",
    "        if not all([x.isgood for x in image_list]):\n",
    "            print('There are bad images in ',\"folder=%f,shot_typeN=%i\"%(folderN,shot_typeN))\n",
    "        else:\n",
    "            if hasattr(image_list[0],'fit1D_x'):\n",
    "                data.fit1D_x_mean = mean([d.fit1D_x for d in image_list],0)\n",
    "                data.fit1D_x_std = std([d.fit1D_x for d in image_list],0)\n",
    "            if hasattr(image_list[0],'fit1D_y'):\n",
    "                data.fit1D_y_mean = mean([d.fit1D_y for d in image_list],0)\n",
    "                data.fit1D_y_std = std([d.fit1D_y for d in image_list],0)\n",
    "            if hasattr(image_list[0],'fit2D'):\n",
    "                data.fit2D_mean = mean([d.fit2D for d in image_list],0)\n",
    "                data.fit2D_std = std([d.fit2D for d in image_list],0)\n",
    "        return (folderN, shot_typeN, data)  \n",
    "    def __call__(self, dataD, lview):\n",
    "        \"\"\" Construct average image\n",
    "        \"\"\"\n",
    "        images_to_avr = []\n",
    "        for folderN, folder_dict in dataD.items():\n",
    "            for shot_typeN, image_list in folder_dict.items():\n",
    "                images_to_avr.append((folderN,shot_typeN,image_list))\n",
    "        res = lview.map(lambda *x: self.avr_image(*x), *zip(*images_to_avr))\n",
    "        res.wait_interactive()\n",
    "        print(''.join([x['stdout'] for x in res.metadata]))\n",
    "        avr_data_list = res.result\n",
    "        avr_data_dict = dict()\n",
    "        for elem in avr_data_list:\n",
    "            if len(elem) != 0:\n",
    "                avr_data_dict[elem[0]] = avr_data_dict.get(elem[0],dict())\n",
    "                avr_data_dict[elem[0]][elem[1]]=elem[2]\n",
    "        return avr_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mod_avrData(avr_dataD, folderN_calib,n_atom_calib, lin_dim_calib):\n",
    "    \"\"\" modifys avr_dataD dictionary with\n",
    "        1 Calibrating x-value(foldeN) in respect with measured (i.e. changes it to Gs or MHz value)\n",
    "        2 Drops from data image attribute (image itself is not needed for futher analizis)\n",
    "        4 Changes numbers of atoms in fits and totals and linear dimentions (line x0 and width) from pixels to teal\n",
    "            size in μm\n",
    "    \"\"\"\n",
    "    def mod_val(key, value):\n",
    "        # modifys data on Natoms and raal_size\n",
    "        val = copy(value)\n",
    "        if key.startswith(\"total\"):\n",
    "            return n_atom_calib(val)\n",
    "        elif key.startswith(\"center_pos\"):\n",
    "            return lin_dim_calib(val)\n",
    "        elif key.startswith(\"fit\"):\n",
    "            val[0] = n_atom_calib(val[0])\n",
    "            val[-1] = n_atom_calib(val[-1])\n",
    "            val[1:-1] = lin_dim_calib(val[1:-1])\n",
    "            return val\n",
    "        else:\n",
    "            return val\n",
    "        \n",
    "        \n",
    "    navrD = dict()\n",
    "    for key in sorted(avr_dataD.keys()):\n",
    "        new_key = folderN_calib(key)\n",
    "        navrD[new_key] = dict()\n",
    "        for shotTypeN, avr_im in avr_dataD[key].items():\n",
    "            navrD[new_key][shotTypeN] = {key: mod_val(key,value) for (key, value) in avr_im.__dict__.items() if key != 'image'}\n",
    "    return navrD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_avr_data(navrD, shot_typeN, attribute, index=None): \n",
    "    \"\"\" Construct data for plot from navrD dictionary\n",
    "        Automaticaly sorts keys and drops a key if there no avr_image for specified shot_typeN\n",
    "        Automaticaly constructs standard deviation error.\n",
    "        If this is not needed just do after d_plot['yerr'] = None\"\"\"\n",
    "    d_plot = dict()\n",
    "    ks = navrD.keys()\n",
    "    ks_f = []\n",
    "    for k in sorted(ks):\n",
    "        if shot_typeN in navrD[k].keys():\n",
    "            ks_f.append(k)\n",
    "        else:\n",
    "            print('navrD has no average image for folderN=%i shot_typeN=%i' % (k,shot_typeN))\n",
    "    d_plot['x'] = array(ks_f)\n",
    "    d_plot['y'] = array([navrD[xx][shot_typeN][attribute][index] if index != None else navrD[xx][shot_typeN][attribute+'_std'] for xx in d_plot['x']])\n",
    "    try:\n",
    "        d_plot['yerr'] = array([navrD[xx][shot_typeN][attribute+'_std'][index] if index != None else navrD[xx][shot_typeN][attribute] for xx in d_plot['x']])\n",
    "    except AttributeError:\n",
    "        d_plot['yerr']=None\n",
    "    return d_plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# old version of above function\n",
    "def get_avr_data_for_plot(avr_dataD, shot_typeN, norm_func, attribute, index=None): \n",
    "    \"\"\" Construct data for plot from avr_dataD dictionary\n",
    "        Automaticaly sorts keys and drops a key if there no avr_image for specified shot_typeN\n",
    "        Automaticaly normalizes 'y' values using norm_func, and constructs standard deviation error.\n",
    "        If this is not needed just do after d_plot['yerr'] = None\"\"\"\n",
    "    d_plot = dict()\n",
    "    ks = avr_dataD.keys()\n",
    "    ks_f = []\n",
    "    for k in sorted(ks):\n",
    "        if shot_typeN in avr_dataD[k].keys():\n",
    "            ks_f.append(k)\n",
    "        else:\n",
    "            print('avr_dataD has no average image for folderN=%i shot_typeN=%i' % (k,shot_typeN))\n",
    "    d_plot['x'] = array(ks_f)\n",
    "    d_plot['y'] = norm_func(array([get_value(avr_dataD[xx][shot_typeN],attribute,index) for xx in d_plot['x']]))\n",
    "    try:\n",
    "        d_plot['yerr'] = norm_func(array([get_value(avr_dataD[xx][shot_typeN],attribute +'_std',index) for xx in d_plot['x']]))\n",
    "    except AttributeError:\n",
    "        d_plot['yerr']=None\n",
    "    return d_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not needed now\n",
    "def get_value(obj, attribute, index):\n",
    "    \"\"\"retruns obj.attibute[index] or obj.attribute if index is not defined\"\"\"\n",
    "    if index != None:\n",
    "        return getattr(obj,attribute)[index]\n",
    "    else:\n",
    "        return getattr(obj,attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# used for constracting data from individual images\n",
    "def constract_data(dictionary, shot_typeN, attribute, index = None):\n",
    "    \"\"\"The most usefull tool. Returns x_data and y_data list already suitable for plotting (i.e. with the same length)\n",
    "    dictionary - the dictionary to extract data from (i.e. dataD or avr_dataD)\n",
    "    shot_typeN - type of the shot sequence (the last number in image name)\n",
    "    attribute - which attribute of data instance to use !!!look at help for Avr_inf and Image_Image and all their \n",
    "    parents\n",
    "    index - if attribute is a list, specifies which paticular data to use\"\"\"\n",
    "    x_data = array([])\n",
    "    y_data = array([])\n",
    "    import collections\n",
    "    for folderN, f_dict in dictionary.items():\n",
    "        if f_dict == {}:\n",
    "            continue\n",
    "        if isinstance(f_dict[shot_typeN], collections.Iterable):\n",
    "            temp_arr = [get_value(elem, attribute, index) for elem in f_dict[shot_typeN]]\n",
    "        else:\n",
    "            temp_arr = [get_value(f_dict[shot_typeN], attribute, index)]\n",
    "        y_data = append(y_data, temp_arr)\n",
    "        x_data = append(x_data, ones(len(temp_arr)) * folderN)\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# not used now\n",
    "def sift(dataD, confidence_interval = 0.1):\n",
    "    \"\"\"Sifts (filters) data on empty images by using average information and comperes centers  of 1D gaussian fits.\n",
    "    If difference is larger the 'confidence_interval' from the average value, the image would be removed from dataD\"\"\"\n",
    "    w = FloatProgress(min=0, max = len(dataD), value=0)\n",
    "    w.description='Sifting in progress...'\n",
    "    display(w)\n",
    "    for folderN, folder_dict in dataD.items():\n",
    "        w.value += 1\n",
    "        for shot_typeN, shot_list in folder_dict.items():\n",
    "            #print(folderN, shot_typeN)\n",
    "            avr_inf = Avr_inf(shot_list, do_fit2D=False)\n",
    "            to_remove = []\n",
    "            for elem in shot_list:\n",
    "                if abs(elem.fit1D_x[1]-avr_inf.fit1D_x[1])/avr_inf.fit1D_x[1] > confidence_interval or \\\n",
    "                    abs(elem.fit1D_y[1]-avr_inf.fit1D_y[1])/avr_inf.fit1D_y[1] > confidence_interval:\n",
    "                        to_remove.append(elem)\n",
    "            for elem in to_remove:\n",
    "                print('remove element',shot_list.index(elem), elem.image_url )\n",
    "                shot_list.remove(elem)\n",
    "    w.bar_style='success'\n",
    "    w.description = 'Sifting Done'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# not used now\n",
    "def single_directory_sift(d_tuple, confidence_interval):\n",
    "    \"\"\"Function to use in parallel sigting\n",
    "    !!! works slower than without parallelism\"\"\"\n",
    "    folderN, folder_dict = d_tuple\n",
    "    temp_dict = dict()\n",
    "    for shot_typeN, shot_list in folder_dict.items():\n",
    "        #print(folderN, shot_typeN)\n",
    "        avr_inf = Avr_inf(shot_list, do_fit2D=False)\n",
    "        to_remove = []\n",
    "        for elem in shot_list:\n",
    "            if abs(elem.x_data_fit[1]-avr_inf.x_data_fit[1])/avr_inf.x_data_fit[1] > confidence_interval or \\\n",
    "                abs(elem.y_data_fit[1]-avr_inf.y_data_fit[1])/avr_inf.y_data_fit[1] > confidence_interval:\n",
    "                    to_remove.append(elem)\n",
    "        for elem in to_remove:\n",
    "            print('remove element',shot_list.index(elem), elem.image_url )\n",
    "            shot_list.remove(elem)\n",
    "    return folderN, folder_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise_avr_image(dictionary, signal_shot, calibration_shot, attribute, index=None, do_fit2D = True):\n",
    "    \"\"\"normalize image from evarage dictionary using attribute[index] value - usually 'total' or 'x_data_fit[0]'\n",
    "        returns constracted dictionary (like what returns 'average_data()' function\"\"\"\n",
    "    norm_data = dict()\n",
    "    w = FloatProgress(min=0, max=len(dictionary),value=0)\n",
    "    w.description='Normalizing in progress...'\n",
    "    display(w)\n",
    "    for folderN, f_dict in dictionary.items():\n",
    "        w.value += 1\n",
    "        norm_data[folderN] = dict()\n",
    "        norm_data[folderN][signal_shot] = Image_Fitted(f_dict[signal_shot].image / \n",
    "                                          get_value(f_dict[calibration_shot],attribute,index),do_fit2D)\n",
    "    w.bar_style='success'\n",
    "    w.description = 'Normalizing Done'\n",
    "    print('Normalization is complited')\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise_individual_image(dictionary, signal_shot, calibration_shot, attribute, index=None, do_fit2D = False):\n",
    "    \"\"\"normalize each image using attribute[index] value - usually 'total' or 'x_data_fit[0]'\n",
    "        returns constracted dictionary (like what returns 'load_data()' function\"\"\"\n",
    "    norm_data = dict()\n",
    "    w = FloatProgress(min=0, max=len(dictionary),value=0)\n",
    "    w.description='Normalizing in progress...'\n",
    "    display(w)\n",
    "    for folderN, f_dict in dictionary.items():\n",
    "        w.value += 1\n",
    "        calibrated_images = []\n",
    "        for s_elem in f_dict[signal_shot]:\n",
    "            c_elems = [c_elem for c_elem in f_dict[calibration_shot] if c_elem.shotN == s_elem.shotN]\n",
    "            if c_elems == []:\n",
    "                print('s_elem.image_url has no calibration image')\n",
    "                continue\n",
    "            calibrated_images = append(calibrated_images, \n",
    "                                       Image_Fitted(s_elem.image / get_value(c_elems[0],attribute,index), do_fit2D))\n",
    "        if calibrated_images != []:\n",
    "            norm_data[folderN] = dict()\n",
    "            norm_data[folderN][signal_shot] = calibrated_images\n",
    "    w.bar_style='success'\n",
    "    w.description = 'Normalizing Done'\n",
    "    print('Normalization is complited')\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class N_atoms:\n",
    "  \"\"\"\n",
    "  Natoms = N_atoms(gain, exp, power, width, delta) - создать объект класса\n",
    "  Natoms(signal) - считает число атомов. по сути в этом месте просто умножение на число\n",
    "  signal - параметр фита\n",
    "  [exposure]=us\n",
    "  [power]=mW\n",
    "  [width]=mm\n",
    "  [delta]=MHz\n",
    "  [gamma]=MHz\n",
    "  [angle]=1\n",
    "  \"\"\"\n",
    "  def __init__(self, gain=100, exposure=300, power=2.7, width=2.27, delta = 0, gamma = 10, angle = 1./225, Isat = 0.18, hw = 6.6*3/0.41*10**(-11)):\n",
    "    self.s = 2*power/3.141592654/width**2/Isat\n",
    "    self.rho = self.s/2/(1+self.s+(2*delta/gamma)**2)\n",
    "    self.p = 9.69*0.001/100/exposure/2.718281828**(3.85/1000*gain)/gamma/hw/angle/self.rho\n",
    "  \n",
    "  def __call__(self, signal):\n",
    "    return signal*self.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def real_size(x, binning=2, pixel_size = 22.3/4):\n",
    "    # returns size of each picset on getted image based on binning and individual pixel size\n",
    "    return x * binning * pixel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_data(data_lists, points):\n",
    "    \"\"\" Drop points from all lists in data_list, mask is constracted using first list in data_lists\"\"\"\n",
    "    mask = array([not(x in points) for x in data_lists[0]])\n",
    "    res = []\n",
    "    for data_list in data_lists:\n",
    "        list.append(res,data_list[mask])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_by_number(data, *numbers):\n",
    "    \"\"\"Drops point by its serial number\"\"\"\n",
    "    for k in data.keys():\n",
    "        if data[k]!=None and type(data[k])!=str:\n",
    "            data[k] = delete(data[k],numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_by_x(data, *points):\n",
    "    \"\"\"Drops point by its 'x' value\"\"\"\n",
    "    mask = array([not(x in points) for x in data['x']])\n",
    "    for k in data.keys():\n",
    "        if data[k]!=None and type(data[k])!=str:\n",
    "            data[k] = data[k][mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data2_sort(x,y):\n",
    "    \"\"\" Sort both array x and y using x-array as criteria\"\"\"\n",
    "    res = array(sorted(zip(x,y), key=lambda x: x[0]))\n",
    "    return res[:,0],res[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default labels for axes\n",
    "x_lbl_default = 'time, ms'\n",
    "y_lbl_default = 'N atoms'\n",
    "\n",
    "# below function to handle different measurement types\n",
    "def parametric_resonance(conf_params):\n",
    "    if 'XAXIS' in conf_params:\n",
    "        xaxis = re.findall('([0-9.]+)(\\w+)',conf_params['XAXIS'])[0]\n",
    "        x_lbl = xaxis[1] \n",
    "        return x_lbl, y_lbl_default,lambda y: y * float(xaxis[0])\n",
    "    else:\n",
    "        x_lbl = 'kHz'\n",
    "        return x_lbl, y_lbl_default, lambda y: y\n",
    "\n",
    "def feshbach(conf_params):\n",
    "    x_lbl = 'magnetic field, Gs'\n",
    "    if 'CONF' not in conf_params: conf_params['CONF'] = 'BH'\n",
    "    if 'OFFSET' not in conf_params: conf_params['OFFSET'] = '0'\n",
    "    return x_lbl, y_lbl_default, lambda x: FB_conf[conf_params['CONF'].upper()][0] * x + \\\n",
    "                                        FB_conf[conf_params['CONF'].upper()][1] * float(conf_params['OFFSET'])\n",
    "    \n",
    "def temperature(conf_params):\n",
    "    y_lbl = 'cloud size, $\\mu$ m'\n",
    "    return x_lbl_default,y_lbl, lambda y: y\n",
    "\n",
    "def clock(conf_params):\n",
    "    x_lbl = 'AOM frequency, MHz'\n",
    "    return x_lbl, y_lbl_default, lambda y: y\n",
    "\n",
    "def as_measurement(conf_params,dirs,folder):\n",
    "    as_folder = [x for x in dirs if x.startswith(re.findall('\\A\\d+\\s+\\w+\\s+(\\d+)', folder)[0])]\n",
    "    if len(as_folder) == 0:\n",
    "        return x_lbl_default, y_lbl_default, lambda x: x\n",
    "    else:\n",
    "        return get_x_calibration(as_folder[0], dirs)\n",
    "\n",
    "def lifetime(conf_params):\n",
    "    return x_lbl_default, y_lbl_default, lambda y: y\n",
    "\n",
    "\n",
    "# dictionary for coils Gauss/Amper values\n",
    "FB_conf = {'BH':(10.2,0.25)}\n",
    "FB_conf['SH']=(FB_conf['BH'][1],FB_conf['BH'][0])\n",
    "\n",
    "# dictionary for normalize functions\n",
    "meas_types = dict()\n",
    "meas_types['FB'] = feshbach\n",
    "meas_types['T']  = temperature\n",
    "meas_types['LT'] = lifetime\n",
    "meas_types['CL'] = clock\n",
    "meas_types['PR'] = parametric_resonance\n",
    "meas_types['AS'] = as_measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_x_calibration(folder,dirs):\n",
    "    \"\"\"\n",
    "    Calibrates x axis depending on measurement type and parameters specified in folder name\n",
    "    \"\"\"\n",
    "    meas_type = re.findall('\\d+\\s+(\\w+)',folder)\n",
    "    conf = re.findall('(\\w+)=(\\S+)+', folder)\n",
    "    conf_params = {key.upper(): value for (key, value) in conf}\n",
    "    if len(meas_type) == 0 or meas_type[0].upper() not in meas_types:\n",
    "        # no calibration for x axis and labels are default ms and Natoms\n",
    "        return x_lbl_default, y_lbl_default, lambda x: x\n",
    "    elif meas_type[0].upper() == 'AS':\n",
    "        return as_measurement(conf_params,dirs,folder)\n",
    "    else:\n",
    "        return meas_types[meas_type[0].upper()](conf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Done importing, module image_processing now')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
